{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import pickle as pkl\n",
    "from attack_op import attack, test_model,parse_param\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "setup_seed(3407)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_cifar10, load_cifar100\n",
    "from models.resnet import load_cifar10_resnet50, load_cifar100_resnet50\n",
    "model = load_cifar100_resnet50()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_param_names = list()\n",
    "for name, param in model.named_parameters():\n",
    "    if not \"bn\" in name and not \"shortcut.1\" in name:\n",
    "        all_param_names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_param_names = all_param_names[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:05<00:00,  2.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0031813300848007203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0007379670739173889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Documents\\GitHub\\Neural-importance\\P1模型训练+归因测试\\参数归因Cifar100\\attack_op.py:79: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  param_totals = np.array(param_totals)\n",
      "100%|██████████| 2/2 [00:00<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003181330442428589\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [6], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m):\n\u001b[0;32m      4\u001b[0m     all_totals\u001b[39m.\u001b[39mappend(attack(train_loaders[i], all_param_names, load_cifar100_resnet50, alpha\u001b[39m=\u001b[39m\u001b[39m0.00001\u001b[39m,num_steps\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,op\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mminus\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m----> 5\u001b[0m     all_totals\u001b[39m.\u001b[39mappend(attack(train_loaders[i], all_param_names, load_cifar100_resnet50, alpha\u001b[39m=\u001b[39;49m\u001b[39m0.00001\u001b[39;49m,num_steps\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m,op\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39madd\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n",
      "File \u001b[1;32md:\\Documents\\GitHub\\Neural-importance\\P1模型训练+归因测试\\参数归因Cifar100\\attack_op.py:70\u001b[0m, in \u001b[0;36mattack\u001b[1;34m(train_loader, params, load_model_func, num_steps, alpha, op)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[39mprint\u001b[39m(total_loss \u001b[39m/\u001b[39m num)\n\u001b[0;32m     69\u001b[0m \u001b[39mfor\u001b[39;00m param \u001b[39min\u001b[39;00m params:\n\u001b[1;32m---> 70\u001b[0m     grad \u001b[39m=\u001b[39m update_param(net, param, alpha, op\u001b[39m=\u001b[39;49mop)\n\u001b[0;32m     71\u001b[0m     \u001b[39mif\u001b[39;00m totals[param] \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m         totals[param] \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m(alpha \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39msign(grad)) \u001b[39m*\u001b[39m grad \u001b[39m/\u001b[39m num\n",
      "File \u001b[1;32md:\\Documents\\GitHub\\Neural-importance\\P1模型训练+归因测试\\参数归因Cifar100\\attack_op.py:48\u001b[0m, in \u001b[0;36mupdate_param\u001b[1;34m(net, param, alpha, op)\u001b[0m\n\u001b[0;32m     44\u001b[0m     weight \u001b[39m=\u001b[39m \u001b[39meval\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mnet.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m param \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.cpu().detach().numpy()\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m \\\n\u001b[0;32m     45\u001b[0m         alpha \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39msign(grad)\n\u001b[0;32m     46\u001b[0m \u001b[39m# weight = eval(\"net.\" + param + \".cpu().detach().numpy()\") + \\\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[39m#     alpha * np.sign(grad)\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m exec(\u001b[39m\"\u001b[39;49m\u001b[39mnet.\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m param \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m = torch.nn.Parameter(torch.from_numpy(weight).to(device))\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     49\u001b[0m \u001b[39mreturn\u001b[39;00m grad\n",
      "File \u001b[1;32m<string>:1\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loaders, test_dataloaders,train_dataloader_all, test_dataloader_all = load_cifar100()\n",
    "all_totals = list()\n",
    "all_totals.append(attack(train_dataloader_all, all_param_names, load_cifar100_resnet50, alpha=0.00001,num_steps=2,op=\"minus\"))\n",
    "all_totals.append(attack(train_dataloader_all, all_param_names, load_cifar100_resnet50, alpha=0.00001,num_steps=4,op=\"add\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_totals_temp = list()\n",
    "from utils import normalization\n",
    "for i in range(0,len(all_totals),2):\n",
    "    total_0 = all_totals[i]\n",
    "    total_1 = all_totals[i+1]\n",
    "    total = dict()\n",
    "    total_values = list()\n",
    "    for key in list(total_0.keys()):\n",
    "        total_values.append(total_0[key] + total_1[key])\n",
    "    total_values = np.array(total_values)\n",
    "    total_values = normalization(abs(total_values))\n",
    "    for key in list(total_0.keys()):\n",
    "        total[key] = total_values[list(total_0.keys()).index(key)]\n",
    "    all_totals_temp.append(total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_totals_clones = all_totals.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_totals = all_totals_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_totals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(all_totals, open(\"weights/op_totals.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thre = 0.25\n",
    "net = load_cifar100_resnet50()\n",
    "param_remove = dict()\n",
    "for param in all_param_names:\n",
    "    param_remove[param] = None\n",
    "for i in range(len(all_totals)):\n",
    "    totals = all_totals[i]\n",
    "    totals = [totals[param] for param in all_param_names]\n",
    "    param_weights = [eval(\"net.\" + parse_param(param) + \".cpu().detach().numpy()\")\n",
    "                     for param in all_param_names]\n",
    "    combine = [np.abs(total * weight) for total, weight in zip(totals, param_weights)]\n",
    "    combine = np.array(combine)\n",
    "    combine_flatten = np.concatenate([combine_.flatten() for combine_ in combine],axis=0)\n",
    "    threshold = np.sort(combine_flatten)[::-1][int(len(combine_flatten) * thre)]\n",
    "    for idx,param in enumerate(all_param_names):\n",
    "        if param_remove[param] is None:\n",
    "            param_remove[param] = combine[idx] > threshold\n",
    "        else:\n",
    "            t = combine[idx] > threshold\n",
    "            param_remove[param] = param_remove[param] | t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 0\n",
    "all_num = 0\n",
    "for param in param_remove:\n",
    "    temp += param_remove[param].sum()\n",
    "    all_num += param_remove[param].size\n",
    "    print(param, param_remove[param].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp / all_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    net = load_cifar100_resnet50()\n",
    "    preds, labels = test_model(net, test_dataloader_all)\n",
    "    print(\"原始准确率\", (preds.argmax(-1) == labels).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    net = load_cifar100_resnet50()\n",
    "    for param in all_param_names:\n",
    "        param_ = parse_param(param)\n",
    "        try:\n",
    "            exec(\"net.\" + param_ + \"[~param_remove[param]] = 0\")\n",
    "        except:\n",
    "            exec(\"net.\" + param_ + \"[~param_remove[param],:] = 0\")\n",
    "    preds, labels = test_model(net, test_dataloader_all)\n",
    "    print(\"现在准确率\", (preds.argmax(-1) == labels).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    net = load_cifar100_resnet50()\n",
    "    for param in all_param_names:\n",
    "        param_ = parse_param(param)\n",
    "        keep_rate = param_remove[param].sum() / param_remove[param].size\n",
    "        weight_flatten = eval(\"net.\" + param_ + \".cpu().detach().numpy()\").flatten()\n",
    "        threshold = np.sort(weight_flatten)[int(len(weight_flatten) * (1 - keep_rate))]\n",
    "        try:\n",
    "            exec(\"net.\" + param_ + \"[eval('net.' + param_ + '.cpu().detach().numpy()') < threshold] = 0\")\n",
    "        except:\n",
    "            exec(\"net.\" + param_ + \"[eval('net.' + param_ + '.cpu().detach().numpy()') < threshold,:] = 0\")\n",
    "    preds, labels = test_model(net, test_dataloader_all)\n",
    "    print(\"对比实验准确率\", (preds.argmax(-1) == labels).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c1050a06cbaeed8b46d187604389f32b45fa537b377a0b8f76b38e0c23b5abbd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
