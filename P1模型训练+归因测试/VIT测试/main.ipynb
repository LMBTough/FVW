{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import VisionTransformer\n",
    "import torch\n",
    "from datasets import load_cifar10\n",
    "from attack import attack,test_model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    model = VisionTransformer(\n",
    "                image_size=(384, 384),\n",
    "                patch_size=(16, 16),\n",
    "                emb_dim=768,\n",
    "                mlp_dim=3072,\n",
    "                num_heads=12,\n",
    "                num_layers=12,\n",
    "                num_classes=10,\n",
    "                attn_dropout_rate=0.0,\n",
    "                dropout_rate=0.1)\n",
    "    state_dict = torch.load(\"weights/best.pth\")[\"state_dict\"]\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_token torch.Size([1, 1, 768])\n",
      "embedding.weight torch.Size([768, 3, 16, 16])\n",
      "embedding.bias torch.Size([768])\n",
      "transformer.pos_embedding.pos_embedding torch.Size([1, 577, 768])\n",
      "transformer.encoder_layers.0.norm1.weight torch.Size([768])\n",
      "transformer.encoder_layers.0.norm1.bias torch.Size([768])\n",
      "transformer.encoder_layers.0.attn.query.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.0.attn.query.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.0.attn.key.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.0.attn.key.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.0.attn.value.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.0.attn.value.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.0.attn.out.weight torch.Size([12, 64, 768])\n",
      "transformer.encoder_layers.0.attn.out.bias torch.Size([768])\n",
      "transformer.encoder_layers.0.norm2.weight torch.Size([768])\n",
      "transformer.encoder_layers.0.norm2.bias torch.Size([768])\n",
      "transformer.encoder_layers.0.mlp.fc1.weight torch.Size([3072, 768])\n",
      "transformer.encoder_layers.0.mlp.fc1.bias torch.Size([3072])\n",
      "transformer.encoder_layers.0.mlp.fc2.weight torch.Size([768, 3072])\n",
      "transformer.encoder_layers.0.mlp.fc2.bias torch.Size([768])\n",
      "transformer.encoder_layers.1.norm1.weight torch.Size([768])\n",
      "transformer.encoder_layers.1.norm1.bias torch.Size([768])\n",
      "transformer.encoder_layers.1.attn.query.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.1.attn.query.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.1.attn.key.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.1.attn.key.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.1.attn.value.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.1.attn.value.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.1.attn.out.weight torch.Size([12, 64, 768])\n",
      "transformer.encoder_layers.1.attn.out.bias torch.Size([768])\n",
      "transformer.encoder_layers.1.norm2.weight torch.Size([768])\n",
      "transformer.encoder_layers.1.norm2.bias torch.Size([768])\n",
      "transformer.encoder_layers.1.mlp.fc1.weight torch.Size([3072, 768])\n",
      "transformer.encoder_layers.1.mlp.fc1.bias torch.Size([3072])\n",
      "transformer.encoder_layers.1.mlp.fc2.weight torch.Size([768, 3072])\n",
      "transformer.encoder_layers.1.mlp.fc2.bias torch.Size([768])\n",
      "transformer.encoder_layers.2.norm1.weight torch.Size([768])\n",
      "transformer.encoder_layers.2.norm1.bias torch.Size([768])\n",
      "transformer.encoder_layers.2.attn.query.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.2.attn.query.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.2.attn.key.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.2.attn.key.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.2.attn.value.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.2.attn.value.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.2.attn.out.weight torch.Size([12, 64, 768])\n",
      "transformer.encoder_layers.2.attn.out.bias torch.Size([768])\n",
      "transformer.encoder_layers.2.norm2.weight torch.Size([768])\n",
      "transformer.encoder_layers.2.norm2.bias torch.Size([768])\n",
      "transformer.encoder_layers.2.mlp.fc1.weight torch.Size([3072, 768])\n",
      "transformer.encoder_layers.2.mlp.fc1.bias torch.Size([3072])\n",
      "transformer.encoder_layers.2.mlp.fc2.weight torch.Size([768, 3072])\n",
      "transformer.encoder_layers.2.mlp.fc2.bias torch.Size([768])\n",
      "transformer.encoder_layers.3.norm1.weight torch.Size([768])\n",
      "transformer.encoder_layers.3.norm1.bias torch.Size([768])\n",
      "transformer.encoder_layers.3.attn.query.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.3.attn.query.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.3.attn.key.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.3.attn.key.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.3.attn.value.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.3.attn.value.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.3.attn.out.weight torch.Size([12, 64, 768])\n",
      "transformer.encoder_layers.3.attn.out.bias torch.Size([768])\n",
      "transformer.encoder_layers.3.norm2.weight torch.Size([768])\n",
      "transformer.encoder_layers.3.norm2.bias torch.Size([768])\n",
      "transformer.encoder_layers.3.mlp.fc1.weight torch.Size([3072, 768])\n",
      "transformer.encoder_layers.3.mlp.fc1.bias torch.Size([3072])\n",
      "transformer.encoder_layers.3.mlp.fc2.weight torch.Size([768, 3072])\n",
      "transformer.encoder_layers.3.mlp.fc2.bias torch.Size([768])\n",
      "transformer.encoder_layers.4.norm1.weight torch.Size([768])\n",
      "transformer.encoder_layers.4.norm1.bias torch.Size([768])\n",
      "transformer.encoder_layers.4.attn.query.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.4.attn.query.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.4.attn.key.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.4.attn.key.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.4.attn.value.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.4.attn.value.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.4.attn.out.weight torch.Size([12, 64, 768])\n",
      "transformer.encoder_layers.4.attn.out.bias torch.Size([768])\n",
      "transformer.encoder_layers.4.norm2.weight torch.Size([768])\n",
      "transformer.encoder_layers.4.norm2.bias torch.Size([768])\n",
      "transformer.encoder_layers.4.mlp.fc1.weight torch.Size([3072, 768])\n",
      "transformer.encoder_layers.4.mlp.fc1.bias torch.Size([3072])\n",
      "transformer.encoder_layers.4.mlp.fc2.weight torch.Size([768, 3072])\n",
      "transformer.encoder_layers.4.mlp.fc2.bias torch.Size([768])\n",
      "transformer.encoder_layers.5.norm1.weight torch.Size([768])\n",
      "transformer.encoder_layers.5.norm1.bias torch.Size([768])\n",
      "transformer.encoder_layers.5.attn.query.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.5.attn.query.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.5.attn.key.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.5.attn.key.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.5.attn.value.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.5.attn.value.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.5.attn.out.weight torch.Size([12, 64, 768])\n",
      "transformer.encoder_layers.5.attn.out.bias torch.Size([768])\n",
      "transformer.encoder_layers.5.norm2.weight torch.Size([768])\n",
      "transformer.encoder_layers.5.norm2.bias torch.Size([768])\n",
      "transformer.encoder_layers.5.mlp.fc1.weight torch.Size([3072, 768])\n",
      "transformer.encoder_layers.5.mlp.fc1.bias torch.Size([3072])\n",
      "transformer.encoder_layers.5.mlp.fc2.weight torch.Size([768, 3072])\n",
      "transformer.encoder_layers.5.mlp.fc2.bias torch.Size([768])\n",
      "transformer.encoder_layers.6.norm1.weight torch.Size([768])\n",
      "transformer.encoder_layers.6.norm1.bias torch.Size([768])\n",
      "transformer.encoder_layers.6.attn.query.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.6.attn.query.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.6.attn.key.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.6.attn.key.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.6.attn.value.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.6.attn.value.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.6.attn.out.weight torch.Size([12, 64, 768])\n",
      "transformer.encoder_layers.6.attn.out.bias torch.Size([768])\n",
      "transformer.encoder_layers.6.norm2.weight torch.Size([768])\n",
      "transformer.encoder_layers.6.norm2.bias torch.Size([768])\n",
      "transformer.encoder_layers.6.mlp.fc1.weight torch.Size([3072, 768])\n",
      "transformer.encoder_layers.6.mlp.fc1.bias torch.Size([3072])\n",
      "transformer.encoder_layers.6.mlp.fc2.weight torch.Size([768, 3072])\n",
      "transformer.encoder_layers.6.mlp.fc2.bias torch.Size([768])\n",
      "transformer.encoder_layers.7.norm1.weight torch.Size([768])\n",
      "transformer.encoder_layers.7.norm1.bias torch.Size([768])\n",
      "transformer.encoder_layers.7.attn.query.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.7.attn.query.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.7.attn.key.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.7.attn.key.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.7.attn.value.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.7.attn.value.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.7.attn.out.weight torch.Size([12, 64, 768])\n",
      "transformer.encoder_layers.7.attn.out.bias torch.Size([768])\n",
      "transformer.encoder_layers.7.norm2.weight torch.Size([768])\n",
      "transformer.encoder_layers.7.norm2.bias torch.Size([768])\n",
      "transformer.encoder_layers.7.mlp.fc1.weight torch.Size([3072, 768])\n",
      "transformer.encoder_layers.7.mlp.fc1.bias torch.Size([3072])\n",
      "transformer.encoder_layers.7.mlp.fc2.weight torch.Size([768, 3072])\n",
      "transformer.encoder_layers.7.mlp.fc2.bias torch.Size([768])\n",
      "transformer.encoder_layers.8.norm1.weight torch.Size([768])\n",
      "transformer.encoder_layers.8.norm1.bias torch.Size([768])\n",
      "transformer.encoder_layers.8.attn.query.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.8.attn.query.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.8.attn.key.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.8.attn.key.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.8.attn.value.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.8.attn.value.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.8.attn.out.weight torch.Size([12, 64, 768])\n",
      "transformer.encoder_layers.8.attn.out.bias torch.Size([768])\n",
      "transformer.encoder_layers.8.norm2.weight torch.Size([768])\n",
      "transformer.encoder_layers.8.norm2.bias torch.Size([768])\n",
      "transformer.encoder_layers.8.mlp.fc1.weight torch.Size([3072, 768])\n",
      "transformer.encoder_layers.8.mlp.fc1.bias torch.Size([3072])\n",
      "transformer.encoder_layers.8.mlp.fc2.weight torch.Size([768, 3072])\n",
      "transformer.encoder_layers.8.mlp.fc2.bias torch.Size([768])\n",
      "transformer.encoder_layers.9.norm1.weight torch.Size([768])\n",
      "transformer.encoder_layers.9.norm1.bias torch.Size([768])\n",
      "transformer.encoder_layers.9.attn.query.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.9.attn.query.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.9.attn.key.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.9.attn.key.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.9.attn.value.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.9.attn.value.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.9.attn.out.weight torch.Size([12, 64, 768])\n",
      "transformer.encoder_layers.9.attn.out.bias torch.Size([768])\n",
      "transformer.encoder_layers.9.norm2.weight torch.Size([768])\n",
      "transformer.encoder_layers.9.norm2.bias torch.Size([768])\n",
      "transformer.encoder_layers.9.mlp.fc1.weight torch.Size([3072, 768])\n",
      "transformer.encoder_layers.9.mlp.fc1.bias torch.Size([3072])\n",
      "transformer.encoder_layers.9.mlp.fc2.weight torch.Size([768, 3072])\n",
      "transformer.encoder_layers.9.mlp.fc2.bias torch.Size([768])\n",
      "transformer.encoder_layers.10.norm1.weight torch.Size([768])\n",
      "transformer.encoder_layers.10.norm1.bias torch.Size([768])\n",
      "transformer.encoder_layers.10.attn.query.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.10.attn.query.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.10.attn.key.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.10.attn.key.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.10.attn.value.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.10.attn.value.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.10.attn.out.weight torch.Size([12, 64, 768])\n",
      "transformer.encoder_layers.10.attn.out.bias torch.Size([768])\n",
      "transformer.encoder_layers.10.norm2.weight torch.Size([768])\n",
      "transformer.encoder_layers.10.norm2.bias torch.Size([768])\n",
      "transformer.encoder_layers.10.mlp.fc1.weight torch.Size([3072, 768])\n",
      "transformer.encoder_layers.10.mlp.fc1.bias torch.Size([3072])\n",
      "transformer.encoder_layers.10.mlp.fc2.weight torch.Size([768, 3072])\n",
      "transformer.encoder_layers.10.mlp.fc2.bias torch.Size([768])\n",
      "transformer.encoder_layers.11.norm1.weight torch.Size([768])\n",
      "transformer.encoder_layers.11.norm1.bias torch.Size([768])\n",
      "transformer.encoder_layers.11.attn.query.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.11.attn.query.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.11.attn.key.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.11.attn.key.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.11.attn.value.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.11.attn.value.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.11.attn.out.weight torch.Size([12, 64, 768])\n",
      "transformer.encoder_layers.11.attn.out.bias torch.Size([768])\n",
      "transformer.encoder_layers.11.norm2.weight torch.Size([768])\n",
      "transformer.encoder_layers.11.norm2.bias torch.Size([768])\n",
      "transformer.encoder_layers.11.mlp.fc1.weight torch.Size([3072, 768])\n",
      "transformer.encoder_layers.11.mlp.fc1.bias torch.Size([3072])\n",
      "transformer.encoder_layers.11.mlp.fc2.weight torch.Size([768, 3072])\n",
      "transformer.encoder_layers.11.mlp.fc2.bias torch.Size([768])\n",
      "transformer.norm.weight torch.Size([768])\n",
      "transformer.norm.bias torch.Size([768])\n",
      "classifier.weight torch.Size([10, 768])\n",
      "classifier.bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_loaders, test_dataloaders, test_dataloader_all = load_cifar10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c06371059844c495c505928b72a305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(9849, 10000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(model,test_dataloader_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54e70f7eba484edfbf2e153985484bdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 976.00 MiB (GPU 0; 16.00 GiB total capacity; 14.81 GiB already allocated; 0 bytes free; 15.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m all_totals \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m()\n\u001b[0;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n\u001b[1;32m----> 3\u001b[0m     all_totals\u001b[39m.\u001b[39mappend(attack(train_loaders[i], [\n\u001b[0;32m      4\u001b[0m                       \u001b[39m\"\u001b[39;49m\u001b[39mtransformer.encoder_layers[10].attn.query\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mtransformer.encoder_layers[10].attn.key\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mtransformer.encoder_layers[10].attn.value\u001b[39;49m\u001b[39m\"\u001b[39;49m], load_model, alpha\u001b[39m=\u001b[39;49m\u001b[39m0.0001\u001b[39;49m))\n",
      "File \u001b[1;32md:\\Documents\\GitHub\\Neural-importance\\P1模型训练+归因测试\\VIT测试\\attack.py:33\u001b[0m, in \u001b[0;36mattack\u001b[1;34m(train_loader, layers, load_model_func, num_steps, alpha)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m tqdm(train_loader,total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(train_loader)):\n\u001b[0;32m     32\u001b[0m     x, y \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(device), y\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> 33\u001b[0m     outputs \u001b[39m=\u001b[39m net(x)\n\u001b[0;32m     34\u001b[0m     loss \u001b[39m=\u001b[39m loss_func(outputs, y)\n\u001b[0;32m     35\u001b[0m     total_loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m+\u001b[39m total_loss\n",
      "File \u001b[1;32mc:\\Users\\Zhang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Documents\\GitHub\\Neural-importance\\P1模型训练+归因测试\\VIT测试\\model.py:205\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    202\u001b[0m emb \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([cls_token, emb], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    204\u001b[0m \u001b[39m# transformer\u001b[39;00m\n\u001b[1;32m--> 205\u001b[0m feat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(emb)\n\u001b[0;32m    207\u001b[0m \u001b[39m# classifier\u001b[39;00m\n\u001b[0;32m    208\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(feat[:, \u001b[39m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Zhang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Documents\\GitHub\\Neural-importance\\P1模型训练+归因测试\\VIT测试\\model.py:151\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    148\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_embedding(x)\n\u001b[0;32m    150\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder_layers:\n\u001b[1;32m--> 151\u001b[0m     out \u001b[39m=\u001b[39m layer(out)\n\u001b[0;32m    153\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(out)\n\u001b[0;32m    154\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\Zhang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Documents\\GitHub\\Neural-importance\\P1模型训练+归因测试\\VIT测试\\model.py:119\u001b[0m, in \u001b[0;36mEncoderBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    117\u001b[0m residual \u001b[39m=\u001b[39m x\n\u001b[0;32m    118\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x)\n\u001b[1;32m--> 119\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(out)\n\u001b[0;32m    120\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout:\n\u001b[0;32m    121\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(out)\n",
      "File \u001b[1;32mc:\\Users\\Zhang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Documents\\GitHub\\Neural-importance\\P1模型训练+归因测试\\VIT测试\\model.py:93\u001b[0m, in \u001b[0;36mSelfAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     90\u001b[0m k \u001b[39m=\u001b[39m k\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m)\n\u001b[0;32m     91\u001b[0m v \u001b[39m=\u001b[39m v\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m attn_weights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(q, k\u001b[39m.\u001b[39;49mtranspose(\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)) \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale\n\u001b[0;32m     94\u001b[0m attn_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(attn_weights, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     95\u001b[0m out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(attn_weights, v)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 976.00 MiB (GPU 0; 16.00 GiB total capacity; 14.81 GiB already allocated; 0 bytes free; 15.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "\n",
    "all_totals = list()\n",
    "for i in range(10):\n",
    "    all_totals.append(attack(train_loaders[i], [\n",
    "                      \"transformer.encoder_layers[10].attn.query\", \"transformer.encoder_layers[10].attn.key\", \"transformer.encoder_layers[10].attn.value\"], load_model, alpha=0.0001))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c1050a06cbaeed8b46d187604389f32b45fa537b377a0b8f76b38e0c23b5abbd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
