{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import VisionTransformer\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    model = VisionTransformer(\n",
    "                image_size=(384, 384),\n",
    "                patch_size=(16, 16),\n",
    "                emb_dim=768,\n",
    "                mlp_dim=3072,\n",
    "                num_heads=12,\n",
    "                num_layers=12,\n",
    "                num_classes=10,\n",
    "                attn_dropout_rate=0.0,\n",
    "                dropout_rate=0.1)\n",
    "    state_dict = torch.load(\"weights/best.pth\")[\"state_dict\"]\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_token torch.Size([1, 1, 768])\n",
      "embedding.weight torch.Size([768, 3, 16, 16])\n",
      "embedding.bias torch.Size([768])\n",
      "transformer.pos_embedding.pos_embedding torch.Size([1, 577, 768])\n",
      "transformer.encoder_layers.0.norm1.weight torch.Size([768])\n",
      "transformer.encoder_layers.0.norm1.bias torch.Size([768])\n",
      "transformer.encoder_layers.0.attn.query.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.0.attn.query.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.0.attn.key.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.0.attn.key.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.0.attn.value.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.0.attn.value.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.0.attn.out.weight torch.Size([12, 64, 768])\n",
      "transformer.encoder_layers.0.attn.out.bias torch.Size([768])\n",
      "transformer.encoder_layers.0.norm2.weight torch.Size([768])\n",
      "transformer.encoder_layers.0.norm2.bias torch.Size([768])\n",
      "transformer.encoder_layers.0.mlp.fc1.weight torch.Size([3072, 768])\n",
      "transformer.encoder_layers.0.mlp.fc1.bias torch.Size([3072])\n",
      "transformer.encoder_layers.0.mlp.fc2.weight torch.Size([768, 3072])\n",
      "transformer.encoder_layers.0.mlp.fc2.bias torch.Size([768])\n",
      "transformer.encoder_layers.1.norm1.weight torch.Size([768])\n",
      "transformer.encoder_layers.1.norm1.bias torch.Size([768])\n",
      "transformer.encoder_layers.1.attn.query.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.1.attn.query.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.1.attn.key.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.1.attn.key.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.1.attn.value.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.1.attn.value.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.1.attn.out.weight torch.Size([12, 64, 768])\n",
      "transformer.encoder_layers.1.attn.out.bias torch.Size([768])\n",
      "transformer.encoder_layers.1.norm2.weight torch.Size([768])\n",
      "transformer.encoder_layers.1.norm2.bias torch.Size([768])\n",
      "transformer.encoder_layers.1.mlp.fc1.weight torch.Size([3072, 768])\n",
      "transformer.encoder_layers.1.mlp.fc1.bias torch.Size([3072])\n",
      "transformer.encoder_layers.1.mlp.fc2.weight torch.Size([768, 3072])\n",
      "transformer.encoder_layers.1.mlp.fc2.bias torch.Size([768])\n",
      "transformer.encoder_layers.2.norm1.weight torch.Size([768])\n",
      "transformer.encoder_layers.2.norm1.bias torch.Size([768])\n",
      "transformer.encoder_layers.2.attn.query.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.2.attn.query.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.2.attn.key.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.2.attn.key.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.2.attn.value.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.2.attn.value.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.2.attn.out.weight torch.Size([12, 64, 768])\n",
      "transformer.encoder_layers.2.attn.out.bias torch.Size([768])\n",
      "transformer.encoder_layers.2.norm2.weight torch.Size([768])\n",
      "transformer.encoder_layers.2.norm2.bias torch.Size([768])\n",
      "transformer.encoder_layers.2.mlp.fc1.weight torch.Size([3072, 768])\n",
      "transformer.encoder_layers.2.mlp.fc1.bias torch.Size([3072])\n",
      "transformer.encoder_layers.2.mlp.fc2.weight torch.Size([768, 3072])\n",
      "transformer.encoder_layers.2.mlp.fc2.bias torch.Size([768])\n",
      "transformer.encoder_layers.3.norm1.weight torch.Size([768])\n",
      "transformer.encoder_layers.3.norm1.bias torch.Size([768])\n",
      "transformer.encoder_layers.3.attn.query.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.3.attn.query.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.3.attn.key.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.3.attn.key.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.3.attn.value.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.3.attn.value.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.3.attn.out.weight torch.Size([12, 64, 768])\n",
      "transformer.encoder_layers.3.attn.out.bias torch.Size([768])\n",
      "transformer.encoder_layers.3.norm2.weight torch.Size([768])\n",
      "transformer.encoder_layers.3.norm2.bias torch.Size([768])\n",
      "transformer.encoder_layers.3.mlp.fc1.weight torch.Size([3072, 768])\n",
      "transformer.encoder_layers.3.mlp.fc1.bias torch.Size([3072])\n",
      "transformer.encoder_layers.3.mlp.fc2.weight torch.Size([768, 3072])\n",
      "transformer.encoder_layers.3.mlp.fc2.bias torch.Size([768])\n",
      "transformer.encoder_layers.4.norm1.weight torch.Size([768])\n",
      "transformer.encoder_layers.4.norm1.bias torch.Size([768])\n",
      "transformer.encoder_layers.4.attn.query.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.4.attn.query.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.4.attn.key.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.4.attn.key.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.4.attn.value.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.4.attn.value.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.4.attn.out.weight torch.Size([12, 64, 768])\n",
      "transformer.encoder_layers.4.attn.out.bias torch.Size([768])\n",
      "transformer.encoder_layers.4.norm2.weight torch.Size([768])\n",
      "transformer.encoder_layers.4.norm2.bias torch.Size([768])\n",
      "transformer.encoder_layers.4.mlp.fc1.weight torch.Size([3072, 768])\n",
      "transformer.encoder_layers.4.mlp.fc1.bias torch.Size([3072])\n",
      "transformer.encoder_layers.4.mlp.fc2.weight torch.Size([768, 3072])\n",
      "transformer.encoder_layers.4.mlp.fc2.bias torch.Size([768])\n",
      "transformer.encoder_layers.5.norm1.weight torch.Size([768])\n",
      "transformer.encoder_layers.5.norm1.bias torch.Size([768])\n",
      "transformer.encoder_layers.5.attn.query.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.5.attn.query.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.5.attn.key.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.5.attn.key.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.5.attn.value.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.5.attn.value.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.5.attn.out.weight torch.Size([12, 64, 768])\n",
      "transformer.encoder_layers.5.attn.out.bias torch.Size([768])\n",
      "transformer.encoder_layers.5.norm2.weight torch.Size([768])\n",
      "transformer.encoder_layers.5.norm2.bias torch.Size([768])\n",
      "transformer.encoder_layers.5.mlp.fc1.weight torch.Size([3072, 768])\n",
      "transformer.encoder_layers.5.mlp.fc1.bias torch.Size([3072])\n",
      "transformer.encoder_layers.5.mlp.fc2.weight torch.Size([768, 3072])\n",
      "transformer.encoder_layers.5.mlp.fc2.bias torch.Size([768])\n",
      "transformer.encoder_layers.6.norm1.weight torch.Size([768])\n",
      "transformer.encoder_layers.6.norm1.bias torch.Size([768])\n",
      "transformer.encoder_layers.6.attn.query.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.6.attn.query.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.6.attn.key.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.6.attn.key.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.6.attn.value.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.6.attn.value.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.6.attn.out.weight torch.Size([12, 64, 768])\n",
      "transformer.encoder_layers.6.attn.out.bias torch.Size([768])\n",
      "transformer.encoder_layers.6.norm2.weight torch.Size([768])\n",
      "transformer.encoder_layers.6.norm2.bias torch.Size([768])\n",
      "transformer.encoder_layers.6.mlp.fc1.weight torch.Size([3072, 768])\n",
      "transformer.encoder_layers.6.mlp.fc1.bias torch.Size([3072])\n",
      "transformer.encoder_layers.6.mlp.fc2.weight torch.Size([768, 3072])\n",
      "transformer.encoder_layers.6.mlp.fc2.bias torch.Size([768])\n",
      "transformer.encoder_layers.7.norm1.weight torch.Size([768])\n",
      "transformer.encoder_layers.7.norm1.bias torch.Size([768])\n",
      "transformer.encoder_layers.7.attn.query.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.7.attn.query.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.7.attn.key.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.7.attn.key.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.7.attn.value.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.7.attn.value.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.7.attn.out.weight torch.Size([12, 64, 768])\n",
      "transformer.encoder_layers.7.attn.out.bias torch.Size([768])\n",
      "transformer.encoder_layers.7.norm2.weight torch.Size([768])\n",
      "transformer.encoder_layers.7.norm2.bias torch.Size([768])\n",
      "transformer.encoder_layers.7.mlp.fc1.weight torch.Size([3072, 768])\n",
      "transformer.encoder_layers.7.mlp.fc1.bias torch.Size([3072])\n",
      "transformer.encoder_layers.7.mlp.fc2.weight torch.Size([768, 3072])\n",
      "transformer.encoder_layers.7.mlp.fc2.bias torch.Size([768])\n",
      "transformer.encoder_layers.8.norm1.weight torch.Size([768])\n",
      "transformer.encoder_layers.8.norm1.bias torch.Size([768])\n",
      "transformer.encoder_layers.8.attn.query.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.8.attn.query.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.8.attn.key.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.8.attn.key.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.8.attn.value.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.8.attn.value.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.8.attn.out.weight torch.Size([12, 64, 768])\n",
      "transformer.encoder_layers.8.attn.out.bias torch.Size([768])\n",
      "transformer.encoder_layers.8.norm2.weight torch.Size([768])\n",
      "transformer.encoder_layers.8.norm2.bias torch.Size([768])\n",
      "transformer.encoder_layers.8.mlp.fc1.weight torch.Size([3072, 768])\n",
      "transformer.encoder_layers.8.mlp.fc1.bias torch.Size([3072])\n",
      "transformer.encoder_layers.8.mlp.fc2.weight torch.Size([768, 3072])\n",
      "transformer.encoder_layers.8.mlp.fc2.bias torch.Size([768])\n",
      "transformer.encoder_layers.9.norm1.weight torch.Size([768])\n",
      "transformer.encoder_layers.9.norm1.bias torch.Size([768])\n",
      "transformer.encoder_layers.9.attn.query.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.9.attn.query.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.9.attn.key.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.9.attn.key.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.9.attn.value.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.9.attn.value.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.9.attn.out.weight torch.Size([12, 64, 768])\n",
      "transformer.encoder_layers.9.attn.out.bias torch.Size([768])\n",
      "transformer.encoder_layers.9.norm2.weight torch.Size([768])\n",
      "transformer.encoder_layers.9.norm2.bias torch.Size([768])\n",
      "transformer.encoder_layers.9.mlp.fc1.weight torch.Size([3072, 768])\n",
      "transformer.encoder_layers.9.mlp.fc1.bias torch.Size([3072])\n",
      "transformer.encoder_layers.9.mlp.fc2.weight torch.Size([768, 3072])\n",
      "transformer.encoder_layers.9.mlp.fc2.bias torch.Size([768])\n",
      "transformer.encoder_layers.10.norm1.weight torch.Size([768])\n",
      "transformer.encoder_layers.10.norm1.bias torch.Size([768])\n",
      "transformer.encoder_layers.10.attn.query.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.10.attn.query.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.10.attn.key.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.10.attn.key.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.10.attn.value.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.10.attn.value.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.10.attn.out.weight torch.Size([12, 64, 768])\n",
      "transformer.encoder_layers.10.attn.out.bias torch.Size([768])\n",
      "transformer.encoder_layers.10.norm2.weight torch.Size([768])\n",
      "transformer.encoder_layers.10.norm2.bias torch.Size([768])\n",
      "transformer.encoder_layers.10.mlp.fc1.weight torch.Size([3072, 768])\n",
      "transformer.encoder_layers.10.mlp.fc1.bias torch.Size([3072])\n",
      "transformer.encoder_layers.10.mlp.fc2.weight torch.Size([768, 3072])\n",
      "transformer.encoder_layers.10.mlp.fc2.bias torch.Size([768])\n",
      "transformer.encoder_layers.11.norm1.weight torch.Size([768])\n",
      "transformer.encoder_layers.11.norm1.bias torch.Size([768])\n",
      "transformer.encoder_layers.11.attn.query.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.11.attn.query.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.11.attn.key.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.11.attn.key.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.11.attn.value.weight torch.Size([768, 12, 64])\n",
      "transformer.encoder_layers.11.attn.value.bias torch.Size([12, 64])\n",
      "transformer.encoder_layers.11.attn.out.weight torch.Size([12, 64, 768])\n",
      "transformer.encoder_layers.11.attn.out.bias torch.Size([768])\n",
      "transformer.encoder_layers.11.norm2.weight torch.Size([768])\n",
      "transformer.encoder_layers.11.norm2.bias torch.Size([768])\n",
      "transformer.encoder_layers.11.mlp.fc1.weight torch.Size([3072, 768])\n",
      "transformer.encoder_layers.11.mlp.fc1.bias torch.Size([3072])\n",
      "transformer.encoder_layers.11.mlp.fc2.weight torch.Size([768, 3072])\n",
      "transformer.encoder_layers.11.mlp.fc2.bias torch.Size([768])\n",
      "transformer.norm.weight torch.Size([768])\n",
      "transformer.norm.bias torch.Size([768])\n",
      "classifier.weight torch.Size([10, 768])\n",
      "classifier.bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c1050a06cbaeed8b46d187604389f32b45fa537b377a0b8f76b38e0c23b5abbd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
